{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been downloaded.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The base URL for the downloads (you need to replace this with the actual base URL)\n",
    "base_url = 'https://lhncbc.nlm.nih.gov/ii/information/MBR/Baselines/2023/'\n",
    "\n",
    "# List of all file names (you need to fill this with the actual file names from the screenshot)\n",
    "# Generating a list of file names based on the given pattern\n",
    "\n",
    "# Define the base of the file names\n",
    "base_name = \"pubmed23n\"\n",
    "\n",
    "# Use a list comprehension to generate the full list of file names\n",
    "file_names = [f\"{base_name}{str(i).zfill(4)}.xml.gz\" for i in range(1, 1167)]\n",
    "\n",
    "# file_names[:10]  # Display the first 10 file names to check if they are correct\n",
    "\n",
    "\n",
    "# Directory where you want to save the downloaded files\n",
    "download_dir = '/data/user_data/jhsia2/dbqa/data/bioasq/annual_zips/'\n",
    "Path(download_dir).mkdir(exist_ok=True)\n",
    "\n",
    "print('All files have been downloaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path of the current notebook (a.ipynb)\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Go up two levels to the 'ragged' directory\n",
    "ragged_path = notebook_path.parent.parent\n",
    "\n",
    "# Add the 'ragged' directory to sys.path\n",
    "if str(ragged_path) not in sys.path:\n",
    "    sys.path.append(str(ragged_path))\n",
    "\n",
    "# Now you can import file_utils\n",
    "from file_utils import save_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jhsia2/ragged')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragged_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://lhncbc.nlm.nih.gov/ii/information/MBR/Baselines/2023.html'\n",
    "\n",
    "# Perform an HTTP GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all 'a' tags with 'href' attribute\n",
    "    a_tags = soup.find_all('a', href=True)\n",
    "\n",
    "    # Extract the URLs from the 'href' attribute\n",
    "    urls = [tag['href'] for tag in a_tags if '.xml.gz' in tag['href']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url in enumerate(urls):\n",
    "    file_name = file_names[i]\n",
    "    local_filename = Path(download_dir) / file_name\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmed23n0001.xml\n",
      "0\n",
      "1 Formate assay in body fluids: application in methanol poisoning. None\n",
      "2 Delineation of the intimate details of the backbone conformation of pyridine nucleotide coenzymes in aqueous solution. None\n",
      "4 Effect of chloroquine on cultured fibroblasts: release of lysosomal hydrolases and inhibition of their uptake. None\n",
      "3 Metal substitutions incarbonic anhydrase: a halide ion probe study. None\n",
      "5 Atomic models for the polypeptide backbones of myohemerythrin and hemerythrin. None\n",
      "6 Studies of oxygen binding energy to hemoglobin molecule. None\n",
      "7 Maturation of the adrenal medulla--IV. Effects of morphine. None\n",
      "8 Comparison between procaine and isocarboxazid metabolism in vitro by a liver microsomal amidase-esterase. None\n",
      "9 Radiochemical assay of glutathione S-epoxide transferase and its enhancement by phenobarbital in rat liver in vivo. None\n",
      "10 Digitoxin metabolism by rat liver microsomes. None\n",
      "11 Identification of adenylate cyclase-coupled beta-adrenergic receptors with radiolabeled beta-adrenergic antagonists. None\n",
      "12 The effect of adrenaline and of alpha- and beta-adrenergic blocking agents on ATP concentration and on incorporation of 32Pi into ATP in rat fat cells. None\n",
      "13 Action of propranolol on mitochondrial functions--effects on energized ion fluxes in the presence of valinomycin. None\n",
      "14 Malathion A and B esterases of mouse liver-I. None\n",
      "15 Increase in acetyl CoA synthetase activity after phenobarbital treatment. None\n",
      "16 Inhibition of aldehyde reductase by acidic metabolites of the biogenic amines. None\n",
      "17 Effects of 5,6-dihydroxytryptamine on tyrosine-hydroxylase activity in central catecholaminergic neurons of the rat. None\n",
      "18 Inhibition of aldehyde reductase isoenzymes in human and rat brain. None\n",
      "19 Antidepressant drugs affect dopamine uptake. None\n",
      "20 Aggregation of blood platelets by adrenaline and its uptake. None\n",
      "21 [Biochemical studies on camomile components/III. In vitro studies about the antipeptic activity of (--)-alpha-bisabolol (author's transl)]. (--)-alpha-Bisabolol has a primary antipeptic action depending on dosage, which is not caused by an alteration of the pH-value. The proteolytic activity of pepsin is reduced by 50 percent through addition of bisabolol in the ratio of 1/0.5. The antipeptic action of bisabolol only occurs in case of direct contact. In case of a previous contact with the substrate, the inhibiting effect is lost.\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# The XML content as a string - for demonstration purposes this will be a truncated version of the XML you provided\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty list to hold the extracted information\n",
    "# articles_info = {}\n",
    "articles_info = []\n",
    "id2title = {}\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_name = file_name.split('.gz')[0]\n",
    "    print(file_name)\n",
    "    file_path = f'/data/user_data/jhsia2/dbqa/data/bioasq/annual_zips/{file_name}'\n",
    "\n",
    "    # Now let's parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Iterate over all articles in the XML\n",
    "    articles = root.findall('.//PubmedArticle')\n",
    "    num_articles = len(articles)\n",
    "    for i, article in enumerate(articles):\n",
    "        # if (i ==3):\n",
    "        #     break\n",
    "        if (i%1000 == 0):\n",
    "            print(i)\n",
    "        # # Find and store the PMID\n",
    "        id = article.findtext('.//PMID')\n",
    "        \n",
    "        # # Find and store the Article Title\n",
    "        title = article.findtext('.//ArticleTitle')\n",
    "        \n",
    "        # # Find and store the Abstract Text\n",
    "        # # Note: This will only get the first AbstractText if there are multiple.\n",
    "        abstract = article.findtext('.//AbstractText')\n",
    "        # print(id, title, abstract)\n",
    "        if title != None and title != '':\n",
    "            articles_info.append({'id': f'{id}_0', 'content': title.strip()})\n",
    "            id2title[id] = title.strip()\n",
    "        if abstract != None and abstract != '':\n",
    "            articles_info.append({'id': f'{id}_1', 'content': abstract.strip()})\n",
    "            break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(id2title, '/data/user_data/jhsia2/dbqa/data/bioasq/id2title.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(articles_info, '/data/user_data/jhsia2/dbqa/data/bioasq/complete_medline_corpus.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('/data/user_data/jhsia2/dbqa/data/bioasq/annual_corpus_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(articles_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
